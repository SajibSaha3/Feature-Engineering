{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f5897eac",
   "metadata": {},
   "source": [
    "# text segmentation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "e7366abc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sentence Tokenization:\n",
      "sentence 0 : Hello world!\n",
      "sentence 1 : This is a test.\n",
      "sentence 2 : Let's see how it works.\n",
      "\n",
      "Word Tokenization:\n",
      "word tokens are like ['Hello', 'world', '!', 'This', 'is', 'a', 'test', '.', 'Let', \"'s\", 'see', 'how', 'it', 'works', '.']\n",
      "word 0 : Hello\n",
      "word 1 : world\n",
      "word 2 : !\n",
      "word 3 : This\n",
      "word 4 : is\n",
      "word 5 : a\n",
      "word 6 : test\n",
      "word 7 : .\n",
      "word 8 : Let\n",
      "word 9 : 's\n",
      "word 10 : see\n",
      "word 11 : how\n",
      "word 12 : it\n",
      "word 13 : works\n",
      "word 14 : .\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "from nltk.tokenize import word_tokenize, sent_tokenize\n",
    "\n",
    "# test input text\n",
    "text = \"Hello world! This is a test. Let's see how it works.\"\n",
    "sen_token = sent_tokenize(text)\n",
    "word_token = word_tokenize(text)\n",
    "\n",
    "# print the results for sentence and word tokenization\n",
    "print(\"Sentence Tokenization:\")\n",
    "for i, sentences in enumerate(sen_token):\n",
    "    print(f'sentence {i} : {sentences}')\n",
    "\n",
    "print(\"\\nWord Tokenization:\")\n",
    "print(\"word tokens are like\", word_token)\n",
    "for i, words in enumerate(word_token):\n",
    "    print(f'word {i} : {words}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "531aef1e",
   "metadata": {},
   "source": [
    "# Lower case conversion and punctuation, url, number and emoji remove"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "0bcdbc97",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Lowercase Text:\n",
      "hey visit right now i  got  amazing deals at  off\n",
      "\n",
      "Tokenized Text:\n",
      "['hey', 'visit', 'right', 'now', 'i', 'got', 'amazing', 'deals', 'at', 'off']\n"
     ]
    }
   ],
   "source": [
    "text2 = \"Hey!! Visit https://example.com right now ðŸ˜„ðŸ”¥ â€” I 017 got 2 amazing deals at 50% @off!! ðŸŽ‰ðŸ’¥\"\n",
    "\n",
    "import re \n",
    "url_pattern = r'https?://[^\\s]+|www\\.[^\\s]+'\n",
    "punctuation_pattern = r'[^\\w\\s]'    \n",
    "digits_pattern = r'\\d+' \n",
    "email_pattern = r'[a-zA-Z0-9_.+-]+@[a-zA-Z0-9-]+\\.[a-zA-Z0-9-.]+'\n",
    "phone_pattern = r'\\b\\d{10}\\b|\\b\\d{3}[-.\\s]??\\d{3}[-.\\s]??\\d{4}\\b'\n",
    "emoji_pattern = r'[\\U0001F600-\\U0001F64F\\U0001F300-\\U0001F5FF\\U0001F680-\\U0001F6FF\\U0001F700-\\U0001F77F\\U00002700-\\U000027BF]'\n",
    "hashtag_pattern = r'#\\w+'\n",
    "mention_pattern = r'@\\w+'\n",
    "\n",
    "\n",
    "text2 = re.sub(url_pattern, '', text2)\n",
    "text2 = re.sub(punctuation_pattern, '', text2)\n",
    "text2 = re.sub(r'\\s+', ' ', text2).strip()\n",
    "text2 = re.sub(digits_pattern, '', text2)\n",
    "text2 = re.sub(email_pattern, '', text2)\n",
    "text2 = re.sub(phone_pattern, '', text2)\n",
    "text2 = re.sub(emoji_pattern, '', text2)\n",
    "text2 = re.sub(hashtag_pattern, '', text2)\n",
    "text2 = re.sub(mention_pattern, '', text2)\n",
    "\n",
    "lower_text = text2.lower().strip()\n",
    "print(\"\\nLowercase Text:\")\n",
    "print(lower_text)\n",
    "\n",
    "# Tokenization\n",
    "tokens = word_tokenize(lower_text)\n",
    "print(\"\\nTokenized Text:\")\n",
    "print(tokens)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a4760c34",
   "metadata": {},
   "source": [
    "# POS TAG"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "8cd91a89",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "POS Tags:\n",
      "The: DT\n",
      "quick: JJ\n",
      "brown: NN\n",
      "fox: NN\n",
      "jumps: VBZ\n",
      "over: IN\n",
      "the: DT\n",
      "lazy: JJ\n",
      "dog: NN\n",
      ".: .\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]     C:\\Users\\Sajib\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
      "[nltk_data]       date!\n"
     ]
    }
   ],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "from nltk.tag import pos_tag\n",
    "nltk.download('averaged_perceptron_tagger')\n",
    "#POS TAG SENTENCE\n",
    "sentence = \"The quick brown fox jumps over the lazy dog.\"\n",
    "tokens = word_tokenize(sentence)\n",
    "pos_tags = pos_tag(tokens)\n",
    "print(\"\\nPOS Tags:\")\n",
    "for word, tag in pos_tags:\n",
    "    print(f'{word}: {tag}')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e5e84d0a",
   "metadata": {},
   "source": [
    "# stop word remove "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eef04b5f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "applying text hey visit right now i  got  amazing deals at  off\n",
      "\n",
      "Filtered Words:\n",
      "['hey', 'visit', 'right', 'got', 'amazing', 'deals']\n",
      "\n",
      "Filtered Sentence:\n",
      "hey visit right got amazing deals\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\Sajib\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "from nltk.corpus import stopwords  \n",
    "nltk.download('stopwords')\n",
    "\n",
    "print(\"applying text\", lower_text)\n",
    "# Remove stop words\n",
    "stop_words = set(stopwords.words('english'))\n",
    "words_token = word_tokenize(lower_text)\n",
    "filtred_word = [words for words in words_token if words not in stop_words]\n",
    "joint_sentence = \" \".join(filtred_word)\n",
    "print(\"\\nFiltered Words:\")\n",
    "print(filtred_word)\n",
    "print(\"\\nFiltered Sentence:\")\n",
    "print(joint_sentence)   "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c688ee7c",
   "metadata": {},
   "source": [
    "# text normalization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "f19f3098",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"helo world! this is a test. let's se how it works. love ya!\""
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text = \"Hellooo worlddd! This is a test. Let's seeeee how it works. luv ya!\"\n",
    "\n",
    "def normalize_text(text):\n",
    "    lower_text = text.lower()\n",
    "    # Remove multiple letters\n",
    "    lower_text = re.sub(r'(.)\\1+', r'\\1', lower_text)  # Replace repeated characters with two occurrences\n",
    "    # Normalize repeated characters\n",
    "    lower_text = re.sub(r'([a-zA-Z])\\1{2,}', r'\\1\\1', lower_text)  # Replace three or more repeated characters with two occurrences\n",
    "    # abbreviations\n",
    "    abbreviations = {\n",
    "        \"u\": \"you\",\n",
    "        \"r\": \"are\",\n",
    "        \"l8r\": \"later\",\n",
    "        \"b4\": \"before\",\n",
    "        \"gr8\": \"great\",\n",
    "        \"omg\": \"oh my god\",\n",
    "        \"btw\": \"by the way\",\n",
    "        \"idk\": \"I don't know\",\n",
    "        \"imo\": \"in my opinion\",\n",
    "        \"tbh\": \"to be honest\",\n",
    "        \"thx\": \"thanks\",\n",
    "        \"pls\": \"please\",\n",
    "        \"luv\": \"love\"\n",
    "    }\n",
    "    for abbr, full in abbreviations.items():\n",
    "        lower_text = re.sub(r'\\b' + re.escape(abbr) + r'\\b', full, lower_text)\n",
    "    return lower_text\n",
    "normalize_text(text)\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9e5556f8",
   "metadata": {},
   "source": [
    "# lemmatization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "35edac6d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Stemmed Sentence:\n",
      "the cat are play with the ball . the dog are bark at the moon . and fill troubl\n",
      "\n",
      "Lemmatized Sentence:\n",
      "The cat are playing with the ball . The dog are barking at the moon . and filled troubled\n",
      "\n",
      "Stemmed Words:\n",
      "['the', 'cat', 'are', 'play', 'with', 'the', 'ball', '.', 'the', 'dog', 'are', 'bark', 'at', 'the', 'moon', '.', 'and', 'fill', 'troubl']\n",
      "The -> the\n",
      "cats -> cat\n",
      "are -> are\n",
      "playing -> play\n",
      "with -> with\n",
      "the -> the\n",
      "ball -> ball\n",
      ". -> .\n",
      "The -> the\n",
      "dogs -> dog\n",
      "are -> are\n",
      "barking -> bark\n",
      "at -> at\n",
      "the -> the\n",
      "moon -> moon\n",
      ". -> .\n",
      "and -> and\n",
      "filled -> fill\n",
      "troubled -> troubl\n",
      "\n",
      "Lemmatized Words:\n",
      "The -> The\n",
      "cats -> cat\n",
      "are -> are\n",
      "playing -> playing\n",
      "with -> with\n",
      "the -> the\n",
      "ball -> ball\n",
      ". -> .\n",
      "The -> The\n",
      "dogs -> dog\n",
      "are -> are\n",
      "barking -> barking\n",
      "at -> at\n",
      "the -> the\n",
      "moon -> moon\n",
      ". -> .\n",
      "and -> and\n",
      "filled -> filled\n",
      "troubled -> troubled\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\Sajib\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\Sajib\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "from nltk.stem import PorterStemmer\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "nltk.download('wordnet')\n",
    "nltk.download('punkt')\n",
    "\n",
    "text = \"The cats are playing with the ball. The dogs are barking at the moon. and filled troubled\"\n",
    "tokens = word_tokenize(text)\n",
    "stemmer = PorterStemmer()\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "stemmed_words = [stemmer.stem(word) for word in tokens]\n",
    "lemmatized_words = [lemmatizer.lemmatize(word) for word in tokens]\n",
    "\n",
    "# join the stemmed and lemmatized words into a single string\n",
    "stemmed_sentence = \" \".join(stemmed_words)\n",
    "lemmatized_sentence = \" \".join(lemmatized_words)\n",
    "print(\"\\nStemmed Sentence:\")\n",
    "print(stemmed_sentence)\n",
    "print(\"\\nLemmatized Sentence:\")\n",
    "print(lemmatized_sentence)\n",
    "\n",
    "print(\"\\nStemmed Words:\")\n",
    "print(stemmed_words)\n",
    "for word, stemmed in zip(tokens, stemmed_words):\n",
    "    print(f'{word} -> {stemmed}')\n",
    "\n",
    "print(\"\\nLemmatized Words:\")\n",
    "for word, lemmatized in zip(tokens, lemmatized_words):\n",
    "    print(f'{word} -> {lemmatized}')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "646410a2",
   "metadata": {},
   "source": [
    "# other lemmatization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "fa0a1e4a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import spacy\n",
    "# nlp = spacy.load(\"en_core_web_sm\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "d98a8b00",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import spacy\n",
    "# nlp = spacy.load(\"en_core_web_sm\")\n",
    "# text = \"The cats are playing with the ball. The dogs are barking at the moon. and filled troubled\"\n",
    "# doc = nlp(text)\n",
    "# for token in doc:\n",
    "#     print(f'{token.text} -> {token.lemma_}')\n",
    "# lemmatized_sentence = \" \".join([token.lemma_ for token in doc])\n",
    "# print(\"\\nLemmatized Sentence using Spacy:\")\n",
    "# print(lemmatized_sentence)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "8109c906",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The cat are playing with the ball . The dog are barking at the moon . and filled troubled\n"
     ]
    }
   ],
   "source": [
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk import word_tokenize\n",
    "\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "text = \"The cats are playing with the ball. The dogs are barking at the moon. and filled troubled\"\n",
    "\n",
    "tokens = word_tokenize(text)\n",
    "\n",
    "lemmatized = [lemmatizer.lemmatize(token) for token in tokens]\n",
    "print(\" \".join(lemmatized))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "71a27366",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
